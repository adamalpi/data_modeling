{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b47fd4ac",
   "metadata": {},
   "source": [
    "# Task 4: Binary Classification - Data Consolidation & Preprocessing\n",
    "\n",
    "Combine the two training datasets (`Training_part1.csv` and `Training_part2.csv`) using the `id` column, preprocess the data, split into train/test sets, and save the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03fdd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "file_path1 = 'data/Training_part1.csv'\n",
    "file_path2 = 'data/Training_part2.csv'\n",
    "\n",
    "# Load the datasets\n",
    "df1 = pd.read_csv(file_path1, sep=\";\")\n",
    "df2 = pd.read_csv(file_path2, sep=\";\")\n",
    "\n",
    "# Display the first few rows and info of each dataframe to understand their structure\n",
    "print(\"First DataFrame (df1) Head:\")\n",
    "print(df1.head())\n",
    "print(\"\\nFirst DataFrame (df1) Info:\")\n",
    "df1.info()\n",
    "\n",
    "print(\"\\nSecond DataFrame (df2) Head:\")\n",
    "print(df2.head())\n",
    "print(\"\\nSecond DataFrame (df2) Info:\")\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5037a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes using the 'id' column\n",
    "# We'll use an inner merge by default, keeping only rows where 'id' exists in both dataframes.\n",
    "merged_df = pd.merge(df1, df2, on='id')\n",
    "\n",
    "# Display the first few rows and info of the merged dataframe\n",
    "print(\"\\nMerged DataFrame Head:\")\n",
    "print(merged_df.head())\n",
    "print(\"\\nMerged DataFrame Info:\")\n",
    "merged_df.info()\n",
    "\n",
    "# Check for the number of unique IDs to ensure the merge was as expected\n",
    "print(f\"\\nNumber of rows in df1: {len(df1)}\")\n",
    "print(f\"Number of rows in df2: {len(df2)}\")\n",
    "print(f\"Number of rows in merged_df: {len(merged_df)}\")\n",
    "print(f\"Number of unique IDs in merged_df: {merged_df['id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29b7ea4",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Before building the classification model, we need to preprocess the data. The steps typically include:\n",
    "\n",
    "1.  **Handling Missing Values:** Identify and decide how to handle any missing data (e.g., imputation, removal).\n",
    "2.  **Feature Selection/Engineering:** Decide which features to use. The `id` column is likely not useful for prediction and should be dropped. We might also create new features if needed.\n",
    "3.  **Encoding Categorical Features:** Convert any non-numeric features into a numeric format suitable for machine learning models (e.g., one-hot encoding).\n",
    "4.  **Feature Scaling:** Scale numerical features to have a similar range (e.g., using StandardScaler) to prevent features with larger values from dominating the model.\n",
    "5.  **Splitting Data:** Divide the dataset into training and testing sets to evaluate the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811eacaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer # Added import\n",
    "\n",
    "# Make a copy to avoid modifying the original merged dataframe\n",
    "processed_df = merged_df.copy()\n",
    "\n",
    "# 1. Handling Missing Values (Initial Check)\n",
    "print(\"\\nMissing values per column (initial):\")\n",
    "print(processed_df.isnull().sum())\n",
    "\n",
    "# 2. Feature Selection\n",
    "# Drop the 'id' column as it's just an identifier\n",
    "processed_df = processed_df.drop('id', axis=1)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "y = processed_df['Class']\n",
    "X = processed_df.drop('Class', axis=1)\n",
    "\n",
    "# 3. Encoding Categorical Features and Data Cleaning\n",
    "print(\"\\nData types of features before encoding/cleaning:\")\n",
    "print(X.dtypes)\n",
    "\n",
    "# Identify categorical columns based on dtype\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "print(f\"\\nIdentified categorical columns for one-hot encoding: {list(categorical_cols)}\")\n",
    "\n",
    "# Apply one-hot encoding to these categorical columns\n",
    "if len(categorical_cols) > 0:\n",
    "    X = pd.get_dummies(X, columns=categorical_cols, drop_first=True, dummy_na=False)\n",
    "    print(\"\\nFeatures after one-hot encoding:\")\n",
    "    print(X.head())\n",
    "    print(\"\\nData types after one-hot encoding:\")\n",
    "    print(X.dtypes)\n",
    "else:\n",
    "    print(\"\\nNo explicit (object/category) categorical columns found for one-hot encoding.\")\n",
    "\n",
    "# Attempt to convert all columns to numeric.\n",
    "# This will turn any remaining non-numeric strings (like 'iii' in a column not caught by select_dtypes or already float/int) into NaN.\n",
    "print(\"\\nAttempting to convert all columns to numeric, coercing errors...\")\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "print(\"\\nData types after coercing all columns to numeric:\")\n",
    "print(X.dtypes)\n",
    "\n",
    "print(\"\\nMissing values after coercing to numeric (includes original NaNs and coerced strings):\")\n",
    "missing_values_summary = X.isnull().sum()\n",
    "print(missing_values_summary[missing_values_summary > 0]) # Show only columns with missing values\n",
    "\n",
    "# 5. Splitting Data (BEFORE imputation and scaling)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "# Impute missing values (those created by 'coerce' or originally present)\n",
    "# Fit imputer ONLY on X_train to prevent data leakage\n",
    "imputer = SimpleImputer(strategy='mean') # Using mean, can be changed to median, most_frequent\n",
    "\n",
    "# Save column names and index before imputation, as imputer returns numpy array\n",
    "X_train_columns = X_train.columns\n",
    "X_train_index = X_train.index\n",
    "X_test_columns = X_test.columns\n",
    "X_test_index = X_test.index\n",
    "\n",
    "print(\"\\nImputing missing values...\")\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_train = pd.DataFrame(X_train, columns=X_train_columns, index=X_train_index)\n",
    "X_test = pd.DataFrame(X_test, columns=X_test_columns, index=X_test_index)\n",
    "\n",
    "print(\"\\nMissing values in X_train after imputation (should be 0):\")\n",
    "print(X_train.isnull().sum().sum())\n",
    "print(\"Missing values in X_test after imputation (should be 0):\")\n",
    "print(X_test.isnull().sum().sum())\n",
    "\n",
    "# 4. Feature Scaling (Now X_train and X_test should be purely numeric and imputed)\n",
    "scaler = StandardScaler()\n",
    "# Fit scaler ONLY on training data\n",
    "X_train_scaled_np = scaler.fit_transform(X_train)\n",
    "# Transform both training and test data\n",
    "X_test_scaled_np = scaler.transform(X_test)\n",
    "\n",
    "# Convert scaled arrays back to DataFrames\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled_np, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled_np, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"\\nScaled Training Data Head:\")\n",
    "print(X_train_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-preprocessed-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features and target for train/test sets\n",
    "train_df = X_train_scaled.copy()\n",
    "train_df['Class'] = y_train.values # Use .values to align correctly if indices differ\n",
    "\n",
    "test_df = X_test_scaled.copy()\n",
    "test_df['Class'] = y_test.values # Use .values to align correctly if indices differ\n",
    "\n",
    "# Add split indicator\n",
    "train_df['split'] = 'train'\n",
    "test_df['split'] = 'test'\n",
    "\n",
    "# Concatenate train and test sets\n",
    "# Use ignore_index=True if the original index isn't important for later steps\n",
    "final_processed_df = pd.concat([train_df, test_df], ignore_index=False) \n",
    "\n",
    "# Define output path\n",
    "output_parquet_path = 'data/preprocessed_data.parquet'\n",
    "\n",
    "# Save to Parquet\n",
    "print(f\"\\nSaving preprocessed data to {output_parquet_path}...\")\n",
    "try:\n",
    "    # Ensure the target 'Class' column is treated appropriately if it's not numeric (e.g., convert 'y'/'n' to 1/0 if needed before saving)\n",
    "    # Example: final_processed_df['Class'] = final_processed_df['Class'].map({'y': 1, 'n': 0})\n",
    "    # Check dtypes before saving\n",
    "    print(\"\\nFinal DataFrame Info before saving:\")\n",
    "    final_processed_df.info()\n",
    "    \n",
    "    final_processed_df.to_parquet(output_parquet_path, index=True) # Save with index\n",
    "    print(f\"Successfully saved preprocessed data to {output_parquet_path}.\")\n",
    "except ImportError:\n",
    "    print(\"\\nError: 'pyarrow' or 'fastparquet' package is required to write to Parquet format.\")\n",
    "    print(\"Please install it using: pip install pyarrow\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while saving to Parquet: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

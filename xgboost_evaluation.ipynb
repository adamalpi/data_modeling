{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBoost Model Evaluation\n",
        "\n",
        "This notebook loads the preprocessed data, retrains the XGBoost model (for self-containment of evaluation), and then evaluates its performance using various metrics, including ROC analysis and threshold tuning for specific class optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "import utils # Import the new utils module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = utils.load_preprocessed_data() # Default path 'data/preprocessed_data.parquet'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate train/test and features/target using utility function\n",
        "X_train_scaled, y_train, X_test_scaled, y_test = utils.split_data_features_target(df)\n",
        "\n",
        "# Convert target variables using utility function\n",
        "y_train = utils.convert_target_variable(y_train)\n",
        "y_test = utils.convert_target_variable(y_test)\n",
        "\n",
        "# The old print statements for shapes are now handled within utils.split_data_features_target()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate class distribution in the training set for scale_pos_weight\n",
        "scale_pos_weight_val = 1 # Default\n",
        "if y_train.dtype == 'int64' or y_train.dtype == 'int32': # Make sure y_train is the converted version\n",
        "    count_class_0 = (y_train == 0).sum()\n",
        "    count_class_1 = (y_train == 1).sum()\n",
        "    print(f\"\\nTraining data class distribution: Class 0 (n): {count_class_0}, Class 1 (y): {count_class_1}\")\n",
        "    if count_class_1 > 0:\n",
        "        scale_pos_weight_val = count_class_0 / count_class_1\n",
        "        print(f\"Calculated scale_pos_weight: {scale_pos_weight_val:.4f}\")\n",
        "    else:\n",
        "        print(\"Warning: No positive class (1) instances in y_train. scale_pos_weight set to 1.\")\n",
        "else:\n",
        "    print(f\"Warning: y_train is not numeric (dtype: {y_train.dtype}) after conversion attempt. scale_pos_weight set to 1.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define and Train XGBoost Classifier\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False, # Recommended to avoid warnings\n",
        "    random_state=42,\n",
        "    n_estimators=100, # Default, can be tuned\n",
        "    scale_pos_weight=scale_pos_weight_val\n",
        ")\n",
        "\n",
        "print(\"\\nTraining XGBoost model for evaluation notebook...\")\n",
        "xgb_clf.fit(X_train_scaled, y_train)\n",
        "print(\"Model training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make Predictions on the test set\n",
        "y_pred_xgb = xgb_clf.predict(X_test_scaled)\n",
        "print(\"Predictions made on the test set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the XGBoost model using the utility function\n",
        "utils.evaluate_model_performance(y_test, y_pred_xgb, X_test_scaled, xgb_clf, \"XGBoost\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Threshold Tuning Analysis for Class n (0)\n",
        "\n",
        "This section explores how changing the classification threshold affects precision, recall, and F1-score, specifically for identifying 'Class n (0)' correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7837c05",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a range of threshold values to test\n",
        "threshold_values = np.arange(0.05, 1.0, 0.05)\n",
        "\n",
        "# Get predicted probabilities for the positive class (Class 1, 'y') from the trained xgb_clf\n",
        "y_pred_proba_for_thresholding = xgb_clf.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "precisions_class0 = []\n",
        "recalls_class0 = []\n",
        "f1s_class0 = []\n",
        "\n",
        "print(\"\\n--- Threshold Tuning for Class n (0) (Uncalibrated Model) ---\")\n",
        "print(f\"{'Threshold':<10} | {'Precision (0)':<15} | {'Recall (0)':<12} | {'F1-score (0)':<12} | {'TP (0)':<7} | {'FP (0)':<7} | {'FN (0)':<7}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for thresh in threshold_values:\n",
        "    # If prob_for_class_1 >= threshold, predict 1 (y), else 0 (n)\n",
        "    y_pred_at_threshold = (y_pred_proba_for_thresholding >= thresh).astype(int)\n",
        "    \n",
        "    # Calculate metrics for Class 0 (label 0)\n",
        "    p, r, f, s = precision_recall_fscore_support(y_test, y_pred_at_threshold, labels=[0, 1], zero_division=0)\n",
        "    \n",
        "    # Confusion matrix for this threshold to get TP, FP, FN for class 0\n",
        "    cm_thresh = confusion_matrix(y_test, y_pred_at_threshold, labels=[0,1])\n",
        "    # For class 0: TP(0) = cm_thresh[0,0], FP(0) = cm_thresh[1,0] (predicted 0, but was 1), FN(0) = cm_thresh[0,1] (predicted 1, but was 0)\n",
        "    tp_c0 = cm_thresh[0,0] if cm_thresh.shape == (2,2) else 0\n",
        "    fp_c0 = cm_thresh[1,0] if cm_thresh.shape == (2,2) else 0 \n",
        "    fn_c0 = cm_thresh[0,1] if cm_thresh.shape == (2,2) else 0\n",
        "\n",
        "    precisions_class0.append(p[0])\n",
        "    recalls_class0.append(r[0])\n",
        "    f1s_class0.append(f[0])\n",
        "    \n",
        "    print(f\"{thresh:<10.2f} | {p[0]:<15.4f} | {r[0]:<12.4f} | {f[0]:<12.4f} | {tp_c0:<7} | {fp_c0:<7} | {fn_c0:<7}\")\n",
        "\n",
        "# Plotting the metrics\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.plot(threshold_values, precisions_class0, label='Precision (Class 0)', marker='o')\n",
        "plt.plot(threshold_values, recalls_class0, label='Recall (Class 0)', marker='x')\n",
        "plt.plot(threshold_values, f1s_class0, label='F1-score (Class 0)', marker='s')\n",
        "plt.title('Precision, Recall, and F1-score for Class n (0) vs. Threshold (Uncalibrated Model)')\n",
        "plt.xlabel('Threshold (Probability for Class y (1))')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(np.round(threshold_values,2))\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Find threshold that maximizes F1-score for Class 0\n",
        "if f1s_class0 and not all(v == 0 for v in f1s_class0): # Check if list is not empty and contains non-zero values\n",
        "    optimal_idx_f1_class0 = np.argmax(f1s_class0)\n",
        "    optimal_threshold_f1_class0 = threshold_values[optimal_idx_f1_class0]\n",
        "    print(f\"\\nOptimal threshold for maximizing F1-score for Class n (0) (Uncalibrated Model): {optimal_threshold_f1_class0:.2f}\")\n",
        "    print(f\"  Precision (Class 0) at this threshold: {precisions_class0[optimal_idx_f1_class0]:.4f}\")\n",
        "    print(f\"  Recall (Class 0) at this threshold: {recalls_class0[optimal_idx_f1_class0]:.4f}\")\n",
        "    print(f\"  F1-score (Class 0) at this threshold: {f1s_class0[optimal_idx_f1_class0]:.4f}\")\n",
        "else:\n",
        "    print(\"\\nCould not determine optimal threshold for F1-score (Class 0) (Uncalibrated Model) as no valid F1 scores were calculated or all were zero.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01814b5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## Feature Importance Analysis\n",
        "#\n",
        "# This section visualizes the importance of each feature in the XGBoost model.\n",
        "\n",
        "# Get feature importances from the trained XGBoost model\n",
        "importances = xgb_clf.feature_importances_\n",
        "feature_names = X_train_scaled.columns # X_train_scaled has the feature names\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
        "\n",
        "# Sort features by importance\n",
        "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
        "\n",
        "print(\"\\n--- Feature Importance Analysis ---\")\n",
        "print(\"Top 10 Feature Importances:\")\n",
        "print(feature_importance_df.head(10))\n",
        "print(\"\\n\") # Add a newline for better separation in output\n",
        "\n",
        "# Plotting the feature importances (e.g., top 20 features)\n",
        "plt.figure(figsize=(12, 8)) # Adjust figure size as needed\n",
        "num_features_to_plot = min(len(feature_importance_df), 20) # Plot top 20 or fewer if not available\n",
        "sns.barplot(x='importance', y='feature', data=feature_importance_df.head(num_features_to_plot), palette='viridis')\n",
        "plt.title(f'Top {num_features_to_plot} Feature Importances - XGBoost')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Features')\n",
        "plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "shap_markdown",
      "metadata": {},
      "source": [
        "## SHAP (SHapley Additive exPlanations) Value Analysis\n",
        "\n",
        "SHAP values provide a way to explain the output of machine learning models by quantifying the contribution of each feature to a particular prediction. This offers more granular insights than global feature importance.\n",
        "\n",
        "- **Summary Plot:** Shows the distribution of SHAP values for each feature, indicating not only the importance but also the direction of the relationship (e.g., whether high values of a feature increase or decrease the prediction).\n",
        "- **Force Plot (for individual predictions):** Illustrates how features contributed to pushing a single prediction away from the base value (average prediction over the training set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shap_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note: If you haven't installed shap, you might need to run: !pip install shap\n",
        "try:\n",
        "    import shap\n",
        "    shap_available = True\n",
        "    shap.initjs() # Initialize JavaScript visualization in the notebook\n",
        "except ImportError:\n",
        "    print(\"SHAP library not found. Please install it to run this section (e.g., pip install shap).\")\n",
        "    shap_available = False\n",
        "\n",
        "if shap_available:\n",
        "    print(\"\\n--- SHAP Value Analysis ---\")\n",
        "    \n",
        "    # Create a SHAP TreeExplainer for the XGBoost model\n",
        "    # For tree-based models like XGBoost, TreeExplainer is more efficient.\n",
        "    explainer = shap.TreeExplainer(xgb_clf)\n",
        "    \n",
        "    # Calculate SHAP values for the test set\n",
        "    # This can take a moment for larger datasets\n",
        "    print(\"Calculating SHAP values for the test set...\")\n",
        "    shap_values = explainer.shap_values(X_test_scaled)\n",
        "    print(\"SHAP values calculated.\")\n",
        "    \n",
        "    # SHAP Summary Plot (Beeswarm)\n",
        "    print(\"\\nSHAP Summary Plot (Beeswarm):\")\n",
        "    shap.summary_plot(shap_values, X_test_scaled, plot_type=\"beeswarm\")\n",
        "    \n",
        "    # SHAP Force Plot for the first instance in the test set\n",
        "    if len(X_test_scaled) > 0:\n",
        "        print(\"\\nSHAP Force Plot for the first test instance:\")\n",
        "        # For binary classification, shap_values can be an array or a list of two arrays (one for each class).\n",
        "        # If it's a list, explainer.expected_value might also be a list.\n",
        "        # We typically explain the probability of the positive class.\n",
        "        expected_value_to_use = explainer.expected_value\n",
        "        shap_values_to_use = shap_values\n",
        "        \n",
        "        if isinstance(explainer.expected_value, (list, np.ndarray)) and len(explainer.expected_value) == 2:\n",
        "            # Common case for binary classifiers from XGBoost when explain_output='probability'\n",
        "            expected_value_to_use = explainer.expected_value[1] # for the positive class (class 1)\n",
        "            shap_values_to_use = shap_values[1] # SHAP values for the positive class\n",
        "            \n",
        "        # If shap_values is a 2D array (instances, features), it's likely for the positive class already\n",
        "        # or for models where explainer.expected_value is a single value.\n",
        "        \n",
        "        shap.force_plot(expected_value_to_use, \n",
        "                        shap_values_to_use[0,:], \n",
        "                        X_test_scaled.iloc[0,:], \n",
        "                        matplotlib=True) # Use matplotlib for better rendering in some environments\n",
        "    else:\n",
        "        print(\"Test set is empty, cannot generate force plot for an instance.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a57bc6a2",
      "metadata": {},
      "source": [
        "## Cross-Validation Evaluation\n",
        "\n",
        "To assess the model's generalization performance more robustly and ensure it's not overfitting to the specific train-test split, we perform k-fold cross-validation on the training data. We'll use Stratified K-Fold to maintain class proportions in each fold.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32af6024",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "# import numpy as np # Already imported\n",
        "\n",
        "# Re-define the XGBoost classifier for cross-validation to ensure fresh state for each fold\n",
        "# Use the same parameters as the main model\n",
        "cv_xgb_clf = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False, \n",
        "    random_state=42,\n",
        "    n_estimators=100, # Or use tuned n_estimators if available from a CV tuning step\n",
        "    scale_pos_weight=scale_pos_weight_val # Use the calculated scale_pos_weight\n",
        ")\n",
        "\n",
        "# Define Stratified K-Fold\n",
        "n_splits_cv = 5 # Number of folds\n",
        "strat_k_fold = StratifiedKFold(n_splits=n_splits_cv, shuffle=True, random_state=42)\n",
        "\n",
        "print(f\"\\n--- Cross-Validation Evaluation ({n_splits_cv}-fold) ---\")\n",
        "\n",
        "# Perform cross-validation for different metrics\n",
        "# Note: X_train_scaled and y_train are used here\n",
        "scoring_metrics = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'roc_auc': 'roc_auc',\n",
        "    'f1': 'f1_weighted' # Using f1_weighted for potentially imbalanced classes\n",
        "}\n",
        "\n",
        "cv_results = {}\n",
        "for metric_name, scorer in scoring_metrics.items():\n",
        "    try:\n",
        "        scores = cross_val_score(cv_xgb_clf, X_train_scaled, y_train, cv=strat_k_fold, scoring=scorer)\n",
        "        cv_results[metric_name] = scores\n",
        "        print(f\"Cross-validation {metric_name.upper()} scores: {scores}\")\n",
        "        print(f\"Mean {metric_name.upper()}: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not calculate cross-validation {metric_name.upper()}. Error: {e}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# You can further analyze or store cv_results if needed\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

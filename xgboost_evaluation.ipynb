{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBoost Model Evaluation\n",
        "\n",
        "This notebook loads the preprocessed data, retrains the XGBoost model (for self-containment of evaluation), and then evaluates its performance using various metrics, including ROC analysis and threshold tuning for specific class optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc, roc_auc_score, precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define input path\n",
        "input_parquet_path = 'data/preprocessed_data.parquet'\n",
        "# Load the preprocessed data\n",
        "print(f\"Loading preprocessed data from {input_parquet_path}...\")\n",
        "try:\n",
        "    df = pd.read_parquet(input_parquet_path)\n",
        "    print(\"Data loaded successfully.\")\n",
        "    df.info()\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {input_parquet_path}. Please ensure '1_consolidate_data.ipynb' has been run.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred while loading the Parquet file: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate train and test sets based on the 'split' column\n",
        "train_df = df[df['split'] == 'train']\n",
        "test_df = df[df['split'] == 'test']\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X_train_scaled = train_df.drop(['Class', 'split'], axis=1)\n",
        "y_train = train_df['Class']\n",
        "\n",
        "X_test_scaled = test_df.drop(['Class', 'split'], axis=1)\n",
        "y_test = test_df['Class']\n",
        "\n",
        "print(f\"Training features shape: {X_train_scaled.shape}, Training target shape: {y_train.shape}\")\n",
        "print(f\"Test features shape: {X_test_scaled.shape}, Test target shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert target variable 'Class' from object ('n'/'y') to numeric (0/1) if necessary\n",
        "if y_train.dtype == 'object':\n",
        "    print(\"\\nConverting target variable 'Class' to numeric (n=0, y=1)...\")\n",
        "    y_train = y_train.map({'n': 0, 'y': 1})\n",
        "    y_test = y_test.map({'n': 0, 'y': 1})\n",
        "    print(\"Target variable converted.\")\n",
        "else:\n",
        "    print(\"\\nTarget variable 'Class' is already numeric or in an unexpected format.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate class distribution in the training set for scale_pos_weight\n",
        "scale_pos_weight_val = 1 # Default\n",
        "if y_train.dtype == 'int64' or y_train.dtype == 'int32':\n",
        "    count_class_0 = (y_train == 0).sum()\n",
        "    count_class_1 = (y_train == 1).sum()\n",
        "    print(f\"\\nTraining data class distribution: Class 0 (n): {count_class_0}, Class 1 (y): {count_class_1}\")\n",
        "    if count_class_1 > 0:\n",
        "        scale_pos_weight_val = count_class_0 / count_class_1\n",
        "        print(f\"Calculated scale_pos_weight: {scale_pos_weight_val:.4f}\")\n",
        "    else:\n",
        "        print(\"Warning: No positive class (1) instances in y_train. scale_pos_weight set to 1.\")\n",
        "else:\n",
        "    print(f\"Warning: y_train is not numeric (dtype: {y_train.dtype}). scale_pos_weight set to 1.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define and Train XGBoost Classifier\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False, # Recommended to avoid warnings\n",
        "    random_state=42,\n",
        "    n_estimators=100, # Default, can be tuned\n",
        "    scale_pos_weight=scale_pos_weight_val\n",
        ")\n",
        "\n",
        "print(\"\\nTraining XGBoost model for evaluation notebook...\")\n",
        "xgb_clf.fit(X_train_scaled, y_train)\n",
        "print(\"Model training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make Predictions on the test set\n",
        "y_pred_xgb = xgb_clf.predict(X_test_scaled)\n",
        "print(\"Predictions made on the test set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(y_true, y_pred, X_features_for_proba, model, model_name):\n",
        "    \"\"\"Calculates, prints, and plots evaluation metrics for a binary classifier.\"\"\"\n",
        "    print(f\"\\n--- {model_name} Evaluation ---\")\n",
        "    \n",
        "    # Accuracy\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    \n",
        "    # Classification Report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    target_names = ['Class n (0)', 'Class y (1)'] if np.all(np.isin(y_true.unique(), [0, 1])) else None\n",
        "    print(classification_report(y_true, y_pred, target_names=target_names, zero_division=0))\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(cm)\n",
        "    \n",
        "    # Plot Confusion Matrix\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    classes = model.classes_ if hasattr(model, 'classes_') else [0, 1]\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n",
        "                xticklabels=classes, \n",
        "                yticklabels=classes)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.show()\n",
        "\n",
        "    # ROC Curve and AUC\n",
        "    if hasattr(model, \"predict_proba\") and X_features_for_proba is not None:\n",
        "        y_pred_proba = model.predict_proba(X_features_for_proba)[:, 1]\n",
        "        fpr, tpr, thresholds_roc = roc_curve(y_true, y_pred_proba)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        print(f\"\\nROC AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title(f'Receiver Operating Characteristic (ROC) - {model_name}')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"\\nROC Curve not available: Model lacks predict_proba or features for probabilities not provided.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the XGBoost model\n",
        "evaluate_model(y_test, y_pred_xgb, X_test_scaled, xgb_clf, \"XGBoost\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Threshold Tuning Analysis for Class n (0)\n",
        "\n",
        "This section explores how changing the classification threshold affects precision, recall, and F1-score, specifically for identifying 'Class n (0)' correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7837c05",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a range of threshold values to test\n",
        "threshold_values = np.arange(0.05, 1.0, 0.05)\n",
        "\n",
        "# Get predicted probabilities for the positive class (Class 1, 'y') from the trained xgb_clf\n",
        "y_pred_proba_for_thresholding = xgb_clf.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "precisions_class0 = []\n",
        "recalls_class0 = []\n",
        "f1s_class0 = []\n",
        "\n",
        "print(\"\\n--- Threshold Tuning for Class n (0) (Uncalibrated Model) ---\")\n",
        "print(f\"{'Threshold':<10} | {'Precision (0)':<15} | {'Recall (0)':<12} | {'F1-score (0)':<12} | {'TP (0)':<7} | {'FP (0)':<7} | {'FN (0)':<7}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for thresh in threshold_values:\n",
        "    # If prob_for_class_1 >= threshold, predict 1 (y), else 0 (n)\n",
        "    y_pred_at_threshold = (y_pred_proba_for_thresholding >= thresh).astype(int)\n",
        "    \n",
        "    # Calculate metrics for Class 0 (label 0)\n",
        "    p, r, f, s = precision_recall_fscore_support(y_test, y_pred_at_threshold, labels=[0, 1], zero_division=0)\n",
        "    \n",
        "    # Confusion matrix for this threshold to get TP, FP, FN for class 0\n",
        "    cm_thresh = confusion_matrix(y_test, y_pred_at_threshold, labels=[0,1])\n",
        "    # For class 0: TP(0) = cm_thresh[0,0], FP(0) = cm_thresh[1,0] (predicted 0, but was 1), FN(0) = cm_thresh[0,1] (predicted 1, but was 0)\n",
        "    tp_c0 = cm_thresh[0,0] if cm_thresh.shape == (2,2) else 0\n",
        "    fp_c0 = cm_thresh[1,0] if cm_thresh.shape == (2,2) else 0 \n",
        "    fn_c0 = cm_thresh[0,1] if cm_thresh.shape == (2,2) else 0\n",
        "\n",
        "    precisions_class0.append(p[0])\n",
        "    recalls_class0.append(r[0])\n",
        "    f1s_class0.append(f[0])\n",
        "    \n",
        "    print(f\"{thresh:<10.2f} | {p[0]:<15.4f} | {r[0]:<12.4f} | {f[0]:<12.4f} | {tp_c0:<7} | {fp_c0:<7} | {fn_c0:<7}\")\n",
        "\n",
        "# Plotting the metrics\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.plot(threshold_values, precisions_class0, label='Precision (Class 0)', marker='o')\n",
        "plt.plot(threshold_values, recalls_class0, label='Recall (Class 0)', marker='x')\n",
        "plt.plot(threshold_values, f1s_class0, label='F1-score (Class 0)', marker='s')\n",
        "plt.title('Precision, Recall, and F1-score for Class n (0) vs. Threshold (Uncalibrated Model)')\n",
        "plt.xlabel('Threshold (Probability for Class y (1))')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(np.round(threshold_values,2))\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Find threshold that maximizes F1-score for Class 0\n",
        "if f1s_class0 and not all(v == 0 for v in f1s_class0): # Check if list is not empty and contains non-zero values\n",
        "    optimal_idx_f1_class0 = np.argmax(f1s_class0)\n",
        "    optimal_threshold_f1_class0 = threshold_values[optimal_idx_f1_class0]\n",
        "    print(f\"\\nOptimal threshold for maximizing F1-score for Class n (0) (Uncalibrated Model): {optimal_threshold_f1_class0:.2f}\")\n",
        "    print(f\"  Precision (Class 0) at this threshold: {precisions_class0[optimal_idx_f1_class0]:.4f}\")\n",
        "    print(f\"  Recall (Class 0) at this threshold: {recalls_class0[optimal_idx_f1_class0]:.4f}\")\n",
        "    print(f\"  F1-score (Class 0) at this threshold: {f1s_class0[optimal_idx_f1_class0]:.4f}\")\n",
        "else:\n",
        "    print(\"\\nCould not determine optimal threshold for F1-score (Class 0) (Uncalibrated Model) as no valid F1 scores were calculated or all were zero.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01814b5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## Feature Importance Analysis\n",
        "#\n",
        "# This section visualizes the importance of each feature in the XGBoost model.\n",
        "\n",
        "# Get feature importances from the trained XGBoost model\n",
        "importances = xgb_clf.feature_importances_\n",
        "feature_names = X_train_scaled.columns # X_train_scaled has the feature names\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
        "\n",
        "# Sort features by importance\n",
        "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
        "\n",
        "print(\"\\n--- Feature Importance Analysis ---\")\n",
        "print(\"Top 10 Feature Importances:\")\n",
        "print(feature_importance_df.head(10))\n",
        "print(\"\\n\") # Add a newline for better separation in output\n",
        "\n",
        "# Plotting the feature importances (e.g., top 20 features)\n",
        "plt.figure(figsize=(12, 8)) # Adjust figure size as needed\n",
        "num_features_to_plot = min(len(feature_importance_df), 20) # Plot top 20 or fewer if not available\n",
        "sns.barplot(x='importance', y='feature', data=feature_importance_df.head(num_features_to_plot), palette='viridis')\n",
        "plt.title(f'Top {num_features_to_plot} Feature Importances - XGBoost')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Features')\n",
        "plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "shap_markdown",
      "metadata": {},
      "source": [
        "## SHAP (SHapley Additive exPlanations) Value Analysis\n",
        "\n",
        "SHAP values provide a way to explain the output of machine learning models by quantifying the contribution of each feature to a particular prediction. This offers more granular insights than global feature importance.\n",
        "\n",
        "- **Summary Plot:** Shows the distribution of SHAP values for each feature, indicating not only the importance but also the direction of the relationship (e.g., whether high values of a feature increase or decrease the prediction).\n",
        "- **Force Plot (for individual predictions):** Illustrates how features contributed to pushing a single prediction away from the base value (average prediction over the training set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shap_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note: If you haven't installed shap, you might need to run: !pip install shap\n",
        "try:\n",
        "    import shap\n",
        "    shap_available = True\n",
        "    shap.initjs() # Initialize JavaScript visualization in the notebook\n",
        "except ImportError:\n",
        "    print(\"SHAP library not found. Please install it to run this section (e.g., pip install shap).\")\n",
        "    shap_available = False\n",
        "\n",
        "if shap_available:\n",
        "    print(\"\\n--- SHAP Value Analysis ---\")\n",
        "    \n",
        "    # Create a SHAP TreeExplainer for the XGBoost model\n",
        "    # For tree-based models like XGBoost, TreeExplainer is more efficient.\n",
        "    explainer = shap.TreeExplainer(xgb_clf)\n",
        "    \n",
        "    # Calculate SHAP values for the test set\n",
        "    # This can take a moment for larger datasets\n",
        "    print(\"Calculating SHAP values for the test set...\")\n",
        "    shap_values = explainer.shap_values(X_test_scaled)\n",
        "    print(\"SHAP values calculated.\")\n",
        "    \n",
        "    # SHAP Summary Plot (Beeswarm)\n",
        "    print(\"\\nSHAP Summary Plot (Beeswarm):\")\n",
        "    shap.summary_plot(shap_values, X_test_scaled, plot_type=\"beeswarm\")\n",
        "    \n",
        "    # SHAP Force Plot for the first instance in the test set\n",
        "    if len(X_test_scaled) > 0:\n",
        "        print(\"\\nSHAP Force Plot for the first test instance:\")\n",
        "        # For binary classification, shap_values can be an array or a list of two arrays (one for each class).\n",
        "        # If it's a list, explainer.expected_value might also be a list.\n",
        "        # We typically explain the probability of the positive class.\n",
        "        expected_value_to_use = explainer.expected_value\n",
        "        shap_values_to_use = shap_values\n",
        "        \n",
        "        if isinstance(explainer.expected_value, (list, np.ndarray)) and len(explainer.expected_value) == 2:\n",
        "            # Common case for binary classifiers from XGBoost when explain_output='probability'\n",
        "            expected_value_to_use = explainer.expected_value[1] # for the positive class (class 1)\n",
        "            shap_values_to_use = shap_values[1] # SHAP values for the positive class\n",
        "            \n",
        "        # If shap_values is a 2D array (instances, features), it's likely for the positive class already\n",
        "        # or for models where explainer.expected_value is a single value.\n",
        "        \n",
        "        shap.force_plot(expected_value_to_use, \n",
        "                        shap_values_to_use[0,:], \n",
        "                        X_test_scaled.iloc[0,:], \n",
        "                        matplotlib=True) # Use matplotlib for better rendering in some environments\n",
        "    else:\n",
        "        print(\"Test set is empty, cannot generate force plot for an instance.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a57bc6a2",
      "metadata": {},
      "source": [
        "## Cross-Validation Evaluation\n",
        "\n",
        "To assess the model's generalization performance more robustly and ensure it's not overfitting to the specific train-test split, we perform k-fold cross-validation on the training data. We'll use Stratified K-Fold to maintain class proportions in each fold.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32af6024",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "# import numpy as np # Already imported\n",
        "\n",
        "# Re-define the XGBoost classifier for cross-validation to ensure fresh state for each fold\n",
        "# Use the same parameters as the main model\n",
        "cv_xgb_clf = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False, \n",
        "    random_state=42,\n",
        "    n_estimators=100, # Or use tuned n_estimators if available from a CV tuning step\n",
        "    scale_pos_weight=scale_pos_weight_val # Use the calculated scale_pos_weight\n",
        ")\n",
        "\n",
        "# Define Stratified K-Fold\n",
        "n_splits_cv = 5 # Number of folds\n",
        "strat_k_fold = StratifiedKFold(n_splits=n_splits_cv, shuffle=True, random_state=42)\n",
        "\n",
        "print(f\"\\n--- Cross-Validation Evaluation ({n_splits_cv}-fold) ---\")\n",
        "\n",
        "# Perform cross-validation for different metrics\n",
        "# Note: X_train_scaled and y_train are used here\n",
        "scoring_metrics = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'roc_auc': 'roc_auc',\n",
        "    'f1': 'f1_weighted' # Using f1_weighted for potentially imbalanced classes\n",
        "}\n",
        "\n",
        "cv_results = {}\n",
        "for metric_name, scorer in scoring_metrics.items():\n",
        "    try:\n",
        "        scores = cross_val_score(cv_xgb_clf, X_train_scaled, y_train, cv=strat_k_fold, scoring=scorer)\n",
        "        cv_results[metric_name] = scores\n",
        "        print(f\"Cross-validation {metric_name.upper()} scores: {scores}\")\n",
        "        print(f\"Mean {metric_name.upper()}: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not calculate cross-validation {metric_name.upper()}. Error: {e}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# You can further analyze or store cv_results if needed\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

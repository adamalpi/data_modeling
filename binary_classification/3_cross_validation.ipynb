{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cv-intro",
   "metadata": {},
   "source": [
    "# Simple Binary Classification with Cross-Validation\n",
    "\n",
    "This notebook loads the preprocessed data saved by `1_consolidate_data.ipynb` and trains/evaluates a Logistic Regression model using K-Fold Cross-Validation.\n",
    "\n",
    "Cross-validation provides a more robust estimate of the model's performance on unseen data compared to a single train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cv-load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data from data/preprocessed_data2.parquet...\n",
      "Data loaded successfully.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3700 entries, 0 to 739\n",
      "Data columns (total 55 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   BIB           3700 non-null   float64\n",
      " 1   FAN           3700 non-null   float64\n",
      " 2   LUK           3700 non-null   float64\n",
      " 3   NUS           3700 non-null   float64\n",
      " 4   SIS           3700 non-null   float64\n",
      " 5   UIN           3700 non-null   float64\n",
      " 6   WET           3700 non-null   float64\n",
      " 7   COD_iii       3700 non-null   float64\n",
      " 8   COD_rrr       3700 non-null   float64\n",
      " 9   COD_uuu       3700 non-null   float64\n",
      " 10  ERG_aaa       3700 non-null   float64\n",
      " 11  ERG_missing   3700 non-null   float64\n",
      " 12  ERG_nnn       3700 non-null   float64\n",
      " 13  ERG_www       3700 non-null   float64\n",
      " 14  GJAH_ii       3700 non-null   float64\n",
      " 15  GJAH_iii      3700 non-null   float64\n",
      " 16  GJAH_missing  3700 non-null   float64\n",
      " 17  GJAH_rrr      3700 non-null   float64\n",
      " 18  MYR_ee        3700 non-null   float64\n",
      " 19  MYR_eee       3700 non-null   float64\n",
      " 20  MYR_fff       3700 non-null   float64\n",
      " 21  MYR_ggg       3700 non-null   float64\n",
      " 22  MYR_hh        3700 non-null   float64\n",
      " 23  MYR_kkk       3700 non-null   float64\n",
      " 24  MYR_lll       3700 non-null   float64\n",
      " 25  MYR_missing   3700 non-null   float64\n",
      " 26  MYR_mmm       3700 non-null   float64\n",
      " 27  MYR_ooo       3700 non-null   float64\n",
      " 28  MYR_sss       3700 non-null   float64\n",
      " 29  MYR_ttt       3700 non-null   float64\n",
      " 30  MYR_yyy       3700 non-null   float64\n",
      " 31  MYR_zzz       3700 non-null   float64\n",
      " 32  PKD_bbb       3700 non-null   float64\n",
      " 33  PKD_ff        3700 non-null   float64\n",
      " 34  PKD_hh        3700 non-null   float64\n",
      " 35  PKD_jjj       3700 non-null   float64\n",
      " 36  PKD_lll       3700 non-null   float64\n",
      " 37  PKD_missing   3700 non-null   float64\n",
      " 38  PKD_ppp       3700 non-null   float64\n",
      " 39  PKD_qqq       3700 non-null   float64\n",
      " 40  PKD_xxx       3700 non-null   float64\n",
      " 41  RAS_f         3700 non-null   float64\n",
      " 42  RAS_missing   3700 non-null   float64\n",
      " 43  RAS_t         3700 non-null   float64\n",
      " 44  TOK_f         3700 non-null   float64\n",
      " 45  TOK_t         3700 non-null   float64\n",
      " 46  VOL_f         3700 non-null   float64\n",
      " 47  VOL_t         3700 non-null   float64\n",
      " 48  KAT_ccc       3700 non-null   float64\n",
      " 49  KAT_ddd       3700 non-null   float64\n",
      " 50  KAT_missing   3700 non-null   float64\n",
      " 51  XIN_f         3700 non-null   float64\n",
      " 52  XIN_t         3700 non-null   float64\n",
      " 53  Class         3700 non-null   object \n",
      " 54  split         3700 non-null   object \n",
      "dtypes: float64(53), object(2)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "df = utils.load_preprocessed_data('data/preprocessed_data2.parquet') # Default path 'data/preprocessed_data.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "301c0287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (2960, 53), Training target shape: (2960,)\n",
      "Test features shape: (740, 53), Test target shape: (740,)\n",
      "\n",
      "Converting target variable 'Class' to numeric (n=0, y=1)...\n",
      "Target variable converted.\n",
      "Value counts:\n",
      " Class\n",
      "1    2739\n",
      "0     221\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Converting target variable 'Class' to numeric (n=0, y=1)...\n",
      "Target variable converted.\n",
      "Value counts:\n",
      " Class\n",
      "1    685\n",
      "0     55\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Separate train/test and features/target using utility function\n",
    "X_train_scaled, y_train, X_test_scaled, y_test = utils.split_data_features_target(df)\n",
    "\n",
    "# Convert target variables using utility function\n",
    "y_train = utils.convert_target_variable(y_train)\n",
    "y_test = utils.convert_target_variable(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cv-prepare-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting target variable 'Class' to numeric (n=0, y=1)...\n",
      "Target variable converted.\n",
      "Value counts:\n",
      " Class\n",
      "1    3424\n",
      "0     276\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Features shape for CV: (3700, 53)\n",
      "Target shape for CV: (3700,)\n"
     ]
    }
   ],
   "source": [
    "# Separate features (X) and target (y) from the entire dataset\n",
    "# Cross-validation will handle the splitting internally.\n",
    "# Note: We are not using the 'split' column here as CV works on the whole dataset \n",
    "# (typically the training portion if it were pre-split, but here we use the full df from parquet).\n",
    "X = df.drop(['Class', 'split'], axis=1) # Assumes 'split' column exists and is not needed for features.\n",
    "y_original = df['Class'] # Get the original target column\n",
    "\n",
    "# Convert target variable using utility function\n",
    "y = utils.convert_target_variable(y_original)\n",
    "\n",
    "print(f\"\\nFeatures shape for CV: {X.shape}\")\n",
    "print(f\"Target shape for CV: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cv-markdown",
   "metadata": {},
   "source": [
    "# Perform Cross-Validation\n",
    "\n",
    "We will use Stratified K-Fold cross-validation to ensure that each fold maintains the same proportion of classes as the original dataset, which is important for potentially imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cv-run",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 5-fold cross-validation...\n",
      "Scores for accuracy: [0.96081081 0.96351351 0.96216216 0.96216216 0.97027027]\n",
      "Mean accuracy: 0.9638 (+/- 0.0034)\n",
      "---\n",
      "Scores for precision: [0.96322489 0.96866097 0.96463932 0.9713056  0.9715505 ]\n",
      "Mean precision: 0.9679 (+/- 0.0034)\n",
      "---\n",
      "Scores for recall: [0.99561404 0.99270073 0.99562044 0.98832117 0.99708029]\n",
      "Mean recall: 0.9939 (+/- 0.0031)\n",
      "---\n",
      "Scores for f1: [0.97915169 0.98053353 0.97988506 0.97973951 0.98414986]\n",
      "Mean f1: 0.9807 (+/- 0.0018)\n",
      "---\n",
      "Cross-validation complete.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "log_reg_cv = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "# Using StratifiedKFold for classification tasks, especially if the target is imbalanced\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, zero_division=0),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f1': make_scorer(f1_score)\n",
    "}\n",
    "\n",
    "# Perform cross-validation for multiple scores\n",
    "print(\"Performing 5-fold cross-validation...\")\n",
    "cv_scores = {}\n",
    "for metric_name, scorer in scoring.items():\n",
    "    scores = cross_val_score(log_reg_cv, X, y, cv=cv_strategy, scoring=scorer)\n",
    "    cv_scores[metric_name] = scores\n",
    "    print(f\"Scores for {metric_name}: {scores}\")\n",
    "    print(f\"Mean {metric_name}: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")\n",
    "    print(\"---\")\n",
    "\n",
    "print(\"Cross-validation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cv-interpretation",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "\n",
    "The results above show the performance for each of the 5 folds and the mean (+/- standard deviation) across the folds for accuracy, precision, recall, and F1-score.\n",
    "\n",
    "- **Mean Score:** Gives an average estimate of the model's performance which is high, but this is a binary classification on a very imbalanced data.\n",
    "- **Standard Deviation:** Indicates the variability of the performance across different folds. A lower standard deviation suggests more consistent performance. In our case the std is low which indicates that the data might be stable, but also that the model might generalize well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

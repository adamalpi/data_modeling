{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b47fd4ac",
   "metadata": {},
   "source": [
    "# Data Consolidation & Preprocessing\n",
    "\n",
    "Combine the two training datasets (`Training_part1.csv` and `Training_part2.csv`) using the `id` column, preprocess the data, split into train/test sets, and save the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b03fdd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38356c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First DataFrame (df1) Head:\n",
      "   BIB  COD  ERG    FAN GJAH    LUK  MYR        NUS  PKD  RAS  id\n",
      "0  160  iii  www   80.0  iii    5.0  eee   800000.0  xxx    t   0\n",
      "1  153  uuu  aaa  200.0  rrr    0.0  mmm  2000000.0  xxx  NaN   1\n",
      "2    5  iii  www   96.0  iii   19.0   hh   960000.0   hh    t   2\n",
      "3    9  iii  www    0.0  iii  120.0  kkk        0.0  qqq  NaN   3\n",
      "4   40  iii  www  232.0  iii    0.0  mmm  2320000.0  xxx    f   4\n",
      "\n",
      "First DataFrame (df1) Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4070 entries, 0 to 4069\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   BIB     4070 non-null   int64  \n",
      " 1   COD     4070 non-null   object \n",
      " 2   ERG     4006 non-null   object \n",
      " 3   FAN     3966 non-null   float64\n",
      " 4   GJAH    4006 non-null   object \n",
      " 5   LUK     4070 non-null   float64\n",
      " 6   MYR     4004 non-null   object \n",
      " 7   NUS     3966 non-null   float64\n",
      " 8   PKD     4004 non-null   object \n",
      " 9   RAS     1705 non-null   object \n",
      " 10  id      4070 non-null   int64  \n",
      "dtypes: float64(3), int64(2), object(6)\n",
      "memory usage: 349.9+ KB\n",
      "\n",
      "Second DataFrame (df2) Head:\n",
      "     SIS TOK    UIN VOL  WET  KAT XIN Class  id\n",
      "0  1.750   t  17.92   f    1  ccc   t     n   0\n",
      "1  0.290   f  16.92   f    0  ddd   f     n   1\n",
      "2  0.000   f  31.25   f    1  ddd   t     n   2\n",
      "3  0.335   f  48.17   f    0  ccc   f     n   3\n",
      "4  0.500   t  32.33   f    0  ddd   f     n   4\n",
      "\n",
      "Second DataFrame (df2) Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4070 entries, 0 to 4069\n",
      "Data columns (total 9 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   SIS     4070 non-null   float64\n",
      " 1   TOK     4070 non-null   object \n",
      " 2   UIN     4028 non-null   float64\n",
      " 3   VOL     4070 non-null   object \n",
      " 4   WET     4070 non-null   int64  \n",
      " 5   KAT     4027 non-null   object \n",
      " 6   XIN     4070 non-null   object \n",
      " 7   Class   4070 non-null   object \n",
      " 8   id      4070 non-null   int64  \n",
      "dtypes: float64(2), int64(2), object(5)\n",
      "memory usage: 286.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "file_path1 = 'data/Training_part1.csv'\n",
    "file_path2 = 'data/Training_part2.csv'\n",
    "\n",
    "# Load the datasets\n",
    "df1 = pd.read_csv(file_path1, sep=\";\")\n",
    "df2 = pd.read_csv(file_path2, sep=\";\")\n",
    "\n",
    "# Display the first few rows and info of each dataframe to understand their structure\n",
    "print(\"First DataFrame (df1) Head:\")\n",
    "print(df1.head())\n",
    "print(\"\\nFirst DataFrame (df1) Info:\")\n",
    "df1.info()\n",
    "\n",
    "print(\"\\nSecond DataFrame (df2) Head:\")\n",
    "print(df2.head())\n",
    "print(\"\\nSecond DataFrame (df2) Info:\")\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c42c5c",
   "metadata": {},
   "source": [
    "After having a quick look at the information printed above, we can see that both files contain different features that don't overlap besides `id`.\n",
    "We also see that this is really not a lot of data, something like 4000 samples, and some features are missing data.\n",
    "\n",
    "After digging into the data, I can also confirm that the types are correctly parsed and that the data is not corrupted at first glance, so there is no apparent need to clean the data in that sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "114d4538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if both dataframes contain the same ids before exploring strategies to combine them\n",
    "a = df1.id.unique()\n",
    "b = df2.id.unique()\n",
    "\n",
    "np.array_equal(a, b) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c576ba33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3700 entries, 0 to 3699\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   BIB     3700 non-null   int64  \n",
      " 1   COD     3700 non-null   object \n",
      " 2   ERG     3636 non-null   object \n",
      " 3   FAN     3600 non-null   float64\n",
      " 4   GJAH    3636 non-null   object \n",
      " 5   LUK     3700 non-null   float64\n",
      " 6   MYR     3634 non-null   object \n",
      " 7   NUS     3600 non-null   float64\n",
      " 8   PKD     3634 non-null   object \n",
      " 9   RAS     1555 non-null   object \n",
      " 10  id      3700 non-null   int64  \n",
      "dtypes: float64(3), int64(2), object(6)\n",
      "memory usage: 346.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3700 entries, 0 to 3699\n",
      "Data columns (total 9 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   SIS     3700 non-null   float64\n",
      " 1   TOK     3700 non-null   object \n",
      " 2   UIN     3661 non-null   float64\n",
      " 3   VOL     3700 non-null   object \n",
      " 4   WET     3700 non-null   int64  \n",
      " 5   KAT     3661 non-null   object \n",
      " 6   XIN     3700 non-null   object \n",
      " 7   Class   3700 non-null   object \n",
      " 8   id      3700 non-null   int64  \n",
      "dtypes: float64(2), int64(2), object(5)\n",
      "memory usage: 289.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Check if there are duplicates and remove them\n",
    "df1.drop_duplicates(inplace=True)\n",
    "df2.drop_duplicates(inplace=True)\n",
    "\n",
    "df1.info()\n",
    "df2.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925ab769",
   "metadata": {},
   "source": [
    "Apparently, there are 3700 different ids both present in both csv files.\n",
    "\n",
    "It also seems that there were duplicates in both, but they were exactly the same entries because after removing the duplicates, the data has the same number of entries, 3700 each. This is not common, we were lucky this time ;) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b240e9",
   "metadata": {},
   "source": [
    "## Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5037a5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged DataFrame Head:\n",
      "   BIB  COD  ERG    FAN GJAH    LUK  MYR        NUS  PKD  RAS  id    SIS TOK  \\\n",
      "0  160  iii  www   80.0  iii    5.0  eee   800000.0  xxx    t   0  1.750   t   \n",
      "1  153  uuu  aaa  200.0  rrr    0.0  mmm  2000000.0  xxx  NaN   1  0.290   f   \n",
      "2    5  iii  www   96.0  iii   19.0   hh   960000.0   hh    t   2  0.000   f   \n",
      "3    9  iii  www    0.0  iii  120.0  kkk        0.0  qqq  NaN   3  0.335   f   \n",
      "4   40  iii  www  232.0  iii    0.0  mmm  2320000.0  xxx    f   4  0.500   t   \n",
      "\n",
      "     UIN VOL  WET  KAT XIN Class  \n",
      "0  17.92   f    1  ccc   t     n  \n",
      "1  16.92   f    0  ddd   f     n  \n",
      "2  31.25   f    1  ddd   t     n  \n",
      "3  48.17   f    0  ccc   f     n  \n",
      "4  32.33   f    0  ddd   f     n  \n",
      "\n",
      "Merged DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3700 entries, 0 to 3699\n",
      "Data columns (total 19 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   BIB     3700 non-null   int64  \n",
      " 1   COD     3700 non-null   object \n",
      " 2   ERG     3636 non-null   object \n",
      " 3   FAN     3600 non-null   float64\n",
      " 4   GJAH    3636 non-null   object \n",
      " 5   LUK     3700 non-null   float64\n",
      " 6   MYR     3634 non-null   object \n",
      " 7   NUS     3600 non-null   float64\n",
      " 8   PKD     3634 non-null   object \n",
      " 9   RAS     1555 non-null   object \n",
      " 10  id      3700 non-null   int64  \n",
      " 11  SIS     3700 non-null   float64\n",
      " 12  TOK     3700 non-null   object \n",
      " 13  UIN     3661 non-null   float64\n",
      " 14  VOL     3700 non-null   object \n",
      " 15  WET     3700 non-null   int64  \n",
      " 16  KAT     3661 non-null   object \n",
      " 17  XIN     3700 non-null   object \n",
      " 18  Class   3700 non-null   object \n",
      "dtypes: float64(5), int64(3), object(11)\n",
      "memory usage: 549.3+ KB\n",
      "\n",
      "Number of rows in df1: 3700\n",
      "Number of rows in df2: 3700\n",
      "Number of rows in merged_df: 3700\n",
      "Number of unique IDs in merged_df: 3700\n"
     ]
    }
   ],
   "source": [
    "# Merge the two dataframes using the 'id' column\n",
    "# We'll use an inner merge by default, keeping only rows where 'id' exists in both dataframes.\n",
    "merged_df = pd.merge(df1, df2, on='id')\n",
    "\n",
    "# Display the first few rows and info of the merged dataframe\n",
    "print(\"\\nMerged DataFrame Head:\")\n",
    "print(merged_df.head())\n",
    "print(\"\\nMerged DataFrame Info:\")\n",
    "merged_df.info()\n",
    "\n",
    "# Check for the number of unique IDs to ensure the merge was as expected\n",
    "print(f\"\\nNumber of rows in df1: {len(df1)}\")\n",
    "print(f\"Number of rows in df2: {len(df2)}\")\n",
    "print(f\"Number of rows in merged_df: {len(merged_df)}\")\n",
    "print(f\"Number of unique IDs in merged_df: {merged_df['id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84c4dc4",
   "metadata": {},
   "source": [
    "The merged dataset also contains 3700 samles as expected, so the merging went smooth.\n",
    "\n",
    "As seen before some features are missing data, the Class column (target) is not missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29b7ea4",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Before building the classification model, we need to preprocess the data. The steps typically include:\n",
    "\n",
    "1.  **Handling Missing Values:** Identify and decide how to handle any missing data (e.g., imputation, removal).\n",
    "2.  **Feature Selection/Engineering:** Decide which features to use. The `id` column is likely not useful for prediction and should be dropped. We might also create new features if needed.\n",
    "3.  **Encoding Categorical Features:** Convert any non-numeric features into a numeric format suitable for machine learning models (e.g., one-hot encoding).\n",
    "4.  **Feature Scaling:** Scale numerical features to have a similar range (e.g., using StandardScaler) to prevent features with larger values from dominating the model.\n",
    "5.  **Splitting Data:** Divide the dataset into training and testing sets to evaluate the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24681b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = merged_df.copy()\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "y = processed_df['Class']\n",
    "X = processed_df.drop(['Class', 'id'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fda4368d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set shape: (2960, 17)\n",
      "Test set shape: (740, 17)\n"
     ]
    }
   ],
   "source": [
    "# Since we have some missing values, we can try to impute them to rescue some.\n",
    "\n",
    "# This step might be revisited in the future based on the model taken, some algorithms like XGBoost \n",
    "# support missing data and making imputations might be detrimental for the model in production in \n",
    "# cases there the missing data is actually not random.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36cf4248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set shape: (2960, 53)\n",
      "Test set shape: (740, 53)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BIB</th>\n",
       "      <th>FAN</th>\n",
       "      <th>LUK</th>\n",
       "      <th>NUS</th>\n",
       "      <th>SIS</th>\n",
       "      <th>UIN</th>\n",
       "      <th>WET</th>\n",
       "      <th>COD_iii</th>\n",
       "      <th>COD_rrr</th>\n",
       "      <th>COD_uuu</th>\n",
       "      <th>...</th>\n",
       "      <th>RAS_t</th>\n",
       "      <th>TOK_f</th>\n",
       "      <th>TOK_t</th>\n",
       "      <th>VOL_f</th>\n",
       "      <th>VOL_t</th>\n",
       "      <th>KAT_ccc</th>\n",
       "      <th>KAT_ddd</th>\n",
       "      <th>KAT_missing</th>\n",
       "      <th>XIN_f</th>\n",
       "      <th>XIN_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.637663</td>\n",
       "      <td>-0.027311</td>\n",
       "      <td>-0.256038</td>\n",
       "      <td>-0.027311</td>\n",
       "      <td>-0.712991</td>\n",
       "      <td>-0.962957</td>\n",
       "      <td>-0.463612</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.210390</td>\n",
       "      <td>-0.544392</td>\n",
       "      <td>-0.103849</td>\n",
       "      <td>-0.544392</td>\n",
       "      <td>-0.790900</td>\n",
       "      <td>-0.783750</td>\n",
       "      <td>-0.612822</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.058442</td>\n",
       "      <td>-1.061473</td>\n",
       "      <td>-0.256038</td>\n",
       "      <td>-1.061473</td>\n",
       "      <td>0.924270</td>\n",
       "      <td>0.118656</td>\n",
       "      <td>-0.612822</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.033712</td>\n",
       "      <td>-0.544392</td>\n",
       "      <td>-0.250397</td>\n",
       "      <td>-0.544392</td>\n",
       "      <td>-0.722294</td>\n",
       "      <td>-0.956585</td>\n",
       "      <td>-0.463612</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.248972</td>\n",
       "      <td>-0.440976</td>\n",
       "      <td>-0.256038</td>\n",
       "      <td>-0.440976</td>\n",
       "      <td>-0.625779</td>\n",
       "      <td>-1.082428</td>\n",
       "      <td>0.431652</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        BIB       FAN       LUK       NUS       SIS       UIN       WET  \\\n",
       "0 -0.637663 -0.027311 -0.256038 -0.027311 -0.712991 -0.962957 -0.463612   \n",
       "1  0.210390 -0.544392 -0.103849 -0.544392 -0.790900 -0.783750 -0.612822   \n",
       "2  1.058442 -1.061473 -0.256038 -1.061473  0.924270  0.118656 -0.612822   \n",
       "3  0.033712 -0.544392 -0.250397 -0.544392 -0.722294 -0.956585 -0.463612   \n",
       "4 -0.248972 -0.440976 -0.256038 -0.440976 -0.625779 -1.082428  0.431652   \n",
       "\n",
       "   COD_iii  COD_rrr  COD_uuu  ...  RAS_t  TOK_f  TOK_t  VOL_f  VOL_t  KAT_ccc  \\\n",
       "0      1.0      0.0      0.0  ...    1.0    1.0    0.0    0.0    1.0      0.0   \n",
       "1      1.0      0.0      0.0  ...    0.0    1.0    0.0    0.0    1.0      1.0   \n",
       "2      1.0      0.0      0.0  ...    0.0    0.0    1.0    0.0    1.0      0.0   \n",
       "3      1.0      0.0      0.0  ...    1.0    0.0    1.0    0.0    1.0      1.0   \n",
       "4      1.0      0.0      0.0  ...    0.0    1.0    0.0    0.0    1.0      1.0   \n",
       "\n",
       "   KAT_ddd  KAT_missing  XIN_f  XIN_t  \n",
       "0      1.0          0.0    0.0    1.0  \n",
       "1      0.0          0.0    1.0    0.0  \n",
       "2      1.0          0.0    1.0    0.0  \n",
       "3      0.0          0.0    0.0    1.0  \n",
       "4      0.0          0.0    0.0    1.0  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_features = X_train.select_dtypes(exclude=['object']).columns\n",
    "categorical_features = X_train.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features),\n",
    "])\n",
    "\n",
    "transformed_data_train = preprocessor.fit_transform(X_train)\n",
    "transformed_data_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Get feature names\n",
    "cat_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)\n",
    "feature_names = list(numeric_features) + list(cat_feature_names)\n",
    "\n",
    "# Create transformed DataFrame\n",
    "train_df = pd.DataFrame(transformed_data_train, columns=feature_names)\n",
    "test_df = pd.DataFrame(transformed_data_test, columns=feature_names)\n",
    "\n",
    "print(f\"\\nTraining set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "save-preprocessed-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving preprocessed data to data/preprocessed_data.parquet...\n",
      "\n",
      "Final DataFrame Info before saving:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3700 entries, 0 to 739\n",
      "Data columns (total 55 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   BIB           3700 non-null   float64\n",
      " 1   FAN           3700 non-null   float64\n",
      " 2   LUK           3700 non-null   float64\n",
      " 3   NUS           3700 non-null   float64\n",
      " 4   SIS           3700 non-null   float64\n",
      " 5   UIN           3700 non-null   float64\n",
      " 6   WET           3700 non-null   float64\n",
      " 7   COD_iii       3700 non-null   float64\n",
      " 8   COD_rrr       3700 non-null   float64\n",
      " 9   COD_uuu       3700 non-null   float64\n",
      " 10  ERG_aaa       3700 non-null   float64\n",
      " 11  ERG_missing   3700 non-null   float64\n",
      " 12  ERG_nnn       3700 non-null   float64\n",
      " 13  ERG_www       3700 non-null   float64\n",
      " 14  GJAH_ii       3700 non-null   float64\n",
      " 15  GJAH_iii      3700 non-null   float64\n",
      " 16  GJAH_missing  3700 non-null   float64\n",
      " 17  GJAH_rrr      3700 non-null   float64\n",
      " 18  MYR_ee        3700 non-null   float64\n",
      " 19  MYR_eee       3700 non-null   float64\n",
      " 20  MYR_fff       3700 non-null   float64\n",
      " 21  MYR_ggg       3700 non-null   float64\n",
      " 22  MYR_hh        3700 non-null   float64\n",
      " 23  MYR_kkk       3700 non-null   float64\n",
      " 24  MYR_lll       3700 non-null   float64\n",
      " 25  MYR_missing   3700 non-null   float64\n",
      " 26  MYR_mmm       3700 non-null   float64\n",
      " 27  MYR_ooo       3700 non-null   float64\n",
      " 28  MYR_sss       3700 non-null   float64\n",
      " 29  MYR_ttt       3700 non-null   float64\n",
      " 30  MYR_yyy       3700 non-null   float64\n",
      " 31  MYR_zzz       3700 non-null   float64\n",
      " 32  PKD_bbb       3700 non-null   float64\n",
      " 33  PKD_ff        3700 non-null   float64\n",
      " 34  PKD_hh        3700 non-null   float64\n",
      " 35  PKD_jjj       3700 non-null   float64\n",
      " 36  PKD_lll       3700 non-null   float64\n",
      " 37  PKD_missing   3700 non-null   float64\n",
      " 38  PKD_ppp       3700 non-null   float64\n",
      " 39  PKD_qqq       3700 non-null   float64\n",
      " 40  PKD_xxx       3700 non-null   float64\n",
      " 41  RAS_f         3700 non-null   float64\n",
      " 42  RAS_missing   3700 non-null   float64\n",
      " 43  RAS_t         3700 non-null   float64\n",
      " 44  TOK_f         3700 non-null   float64\n",
      " 45  TOK_t         3700 non-null   float64\n",
      " 46  VOL_f         3700 non-null   float64\n",
      " 47  VOL_t         3700 non-null   float64\n",
      " 48  KAT_ccc       3700 non-null   float64\n",
      " 49  KAT_ddd       3700 non-null   float64\n",
      " 50  KAT_missing   3700 non-null   float64\n",
      " 51  XIN_f         3700 non-null   float64\n",
      " 52  XIN_t         3700 non-null   float64\n",
      " 53  Class         3700 non-null   object \n",
      " 54  split         3700 non-null   object \n",
      "dtypes: float64(53), object(2)\n",
      "memory usage: 1.6+ MB\n",
      "Successfully saved preprocessed data to data/preprocessed_data.parquet.\n"
     ]
    }
   ],
   "source": [
    "# Combine features and target for train/test sets\n",
    "train_df['Class'] = y_train.values \n",
    "test_df['Class'] = y_test.values\n",
    "\n",
    "# Add split indicator\n",
    "train_df['split'] = 'train'\n",
    "test_df['split'] = 'test'\n",
    "\n",
    "# Concatenate train and test sets\n",
    "final_processed_df = pd.concat([train_df, test_df], ignore_index=False) \n",
    "\n",
    "# Define output path\n",
    "output_parquet_path = 'data/preprocessed_data.parquet'\n",
    "\n",
    "# Save to Parquet\n",
    "print(f\"\\nSaving preprocessed data to {output_parquet_path}...\")\n",
    "try:\n",
    "    # Ensure the target 'Class' column is treated appropriately if it's not numeric (e.g., convert 'y'/'n' to 1/0 if needed before saving)\n",
    "    # Example: final_processed_df['Class'] = final_processed_df['Class'].map({'y': 1, 'n': 0})\n",
    "    # Check dtypes before saving\n",
    "    print(\"\\nFinal DataFrame Info before saving:\")\n",
    "    final_processed_df.info()\n",
    "    \n",
    "    final_processed_df.to_parquet(output_parquet_path, index=True) # Save with index\n",
    "    print(f\"Successfully saved preprocessed data to {output_parquet_path}.\")\n",
    "except ImportError:\n",
    "    print(\"\\nError: 'pyarrow' or 'fastparquet' package is required to write to Parquet format.\")\n",
    "    print(\"Please install it using: pip install pyarrow\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while saving to Parquet: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
